{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nittfHfNvRoS"
      },
      "source": [
        "##### Copyright 2020 Google LLC.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Ptc9JShvY9v"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 The Google Research Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUUSOrEbUw_4"
      },
      "source": [
        "# jax2tex\n",
        "Sam Schoenholz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f83pDTTjXqP-"
      },
      "source": [
        "In debugging JAX code, it is sometimes desireable to understand what is going on inside a transformed function. Right now, as far as I know, the only ways to do this are: to look at the JAXPR of the transformed function, to look at the XLA HLOs, or to step through in the debugger. However, I have often found none of these solutions to be particularly satisfactory. For mathematical functions all of these op-by-op representations throw away a lot of useful structure that we, as humans, rely on to understand calculations (such as the grouping and structure of terms in equations). After all, we have spent thousands of hours training our visual faculties to understand math, why throw that away?\n",
        "\n",
        "I have recently found it useful to look at representations of functions that are closer to mathematical expressions that we are familiar with. It has been especially nice to decide on semantic groupings of variables that make it easier to understand the flow of calculations. To that end, I've been playing around with a small (three function) library `jax2tex` that converts from jax traceable functions to latex. This notebook contains some examples showing how the library works.\n",
        "\n",
        "There are many different choices that one could make when transcribing JAX functions to latex. Here are a few decisions that I made that others might disagree with.\n",
        "\n",
        "1. I have chosen a style that is very explicit in terms of indexing. This is to make it clear what the numpy code is doing.\n",
        "2. There is some ambiguity and clutter due to numpy's flexibility re: singleton dimensions. Therefore, I suppress singleton dimensions. (E.g. dot products of dimensions of size-one are elided). \n",
        "3. To further remove clutter, I remove ops that affect the computation without affecting the math (e.g. type conversion).\n",
        "4. To generate effective latex, you will likely need to hand-annotate the function rather than trying to do the annotation automatically. After all, people have carefully decided how to group terms and the grouping is usually very problem specific.\n",
        "5. Notationally: the prefixes $d$ and $\\delta$ refer to tangent vectors in the forward pass and cotangent vectors in the backward pass respectively.\n",
        "\n",
        "There are still several TODOS that I think would be nice to add.\n",
        "\n",
        "1. Most ops are still not implemented. Let me know if you would like to use jax2tex but are blocked by a specific op.\n",
        "2. It would be nice to do some very simple algebraic simplification to further reduce clutter.\n",
        "3. If people start using this seriously, it should be more rigorously tested.\n",
        "\n",
        "With this having been said, let's continue to the demo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "K5CVBzOwA-Tt"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install subversion\n",
        "!svn export https://github.com/google-research/google-research/trunk/jax2tex\n",
        "!pip install jax2tex/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Kk5hz42718MK"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "import jax2tex as j2t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TO8L1KKhcabf"
      },
      "source": [
        "Let us first define a simple one-hidden layer linear MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4AFsQ6eY2JoJ"
      },
      "outputs": [],
      "source": [
        "def f(x, y):\n",
        "  return x * (x - y) / (x + y) + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cQfNnLWIcf25"
      },
      "source": [
        "We can then look at both the jaxpr for the function. For now we'll use scalar dummy inputs of `x = 1` and `y = 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 139
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 293,
          "status": "ok",
          "timestamp": 1597960631650,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "vsSTc10Sclhw",
        "outputId": "af9c0fac-ec38-4707-844a-f80257283ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ lambda  ; a b.\n",
            "  let c = sub a b\n",
            "      d = mul a c\n",
            "      e = add a b\n",
            "      f = div d e\n",
            "      g = add f b\n",
            "  in (g,) }\n"
          ]
        }
      ],
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "print(make_jaxpr(f)(1., 1.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b2DmpxVcctUc"
      },
      "source": [
        "We can also look at the latex:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 35
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 292,
          "status": "ok",
          "timestamp": 1597960631951,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "--1AF-u62eHc",
        "outputId": "b8eff690-be87-4e9b-a23a-83a4654d03a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f \u0026= {x\\left(x - y\\right) \\over x + y} + y\n"
          ]
        }
      ],
      "source": [
        "print(j2t.jax2tex(f, 1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZHj5Zpehg9VS"
      },
      "source": [
        "$$f = {x\\left(x - y\\right) \\over x + y} + y$$\n",
        "\n",
        "It is frequently useful to define intermediate variables,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 52
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 294,
          "status": "ok",
          "timestamp": 1597960632254,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "_ie-EU0phILx",
        "outputId": "7fcbc62c-1718-4283-d8e4-9830240c4e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z \u0026= x + y\\\\\n",
            "f \u0026= {x\\left(x - y\\right) \\over z} + y\n"
          ]
        }
      ],
      "source": [
        "def f(x, y):\n",
        "  z = j2t.tex_var(x + y, 'z')\n",
        "  return x * (x - y) / z + y\n",
        "\n",
        "print(j2t.jax2tex(f, 1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jS3Rr6p5hSKy"
      },
      "source": [
        "$$z = x + y\\\\\n",
        "f = {x\\left(x - y\\right) \\over z} + y$$\n",
        "\n",
        "Now let's use this to get insight into what is happening inside a gradient calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 55
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 297,
          "status": "ok",
          "timestamp": 1597960632565,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "1M-Vim-aeJ_7",
        "outputId": "5133f9fd-bf0d-44a2-f279-d9308eef3163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\delta x \u0026= -1.0{\\left(x + y\\right)}^{-2}x\\left(x - y\\right) + {1.0 \\over x + y}x + {1.0 \\over x + y}\\left(x - y\\right)\n"
          ]
        }
      ],
      "source": [
        "from jax import grad\n",
        "\n",
        "@j2t.bind_names\n",
        "def f(x, y):\n",
        "  return x * (x - y) / (x + y) + y\n",
        "\n",
        "print(j2t.jax2tex(grad(f), 1., 1.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q2m1j3-5iHNW"
      },
      "source": [
        "$$\n",
        "\\delta x = -1.0{\\left(x + y\\right)}^{-2}x\\left(x - y\\right) + {1.0 \\over x + y}x + {1.0 \\over x + y}\\left(x - y\\right)$$\n",
        "\n",
        "Here we the product rule with the three terms corresponding to the derivative of $(x + y)^{-1}$, $x- y$, and $x$ respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4SxJOavbj1mT"
      },
      "source": [
        "Now we will consider a slightly more realistic example: a simple one-hidden layer MLP with quadratic activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pfCKR1grj068"
      },
      "outputs": [],
      "source": [
        "@j2t.bind_names\n",
        "def mlp(U, V, x):\n",
        "  z1 = j2t.tex_var(x @ U, 'z^1')\n",
        "  y1 = j2t.tex_var(jax.nn.relu(z1), 'y^1')\n",
        "  return j2t.tex_var(y1 @ V, 'z^2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAvRhC-RlJAY"
      },
      "source": [
        "Now we will evaluate the function using dummy inputs and weights. Here we take the input to have shape `[3,]` with readin weights of shape `[3, 2]` and readout weights to a scalar output of shape `[2,]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 69
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 573,
          "status": "ok",
          "timestamp": 1597960769552,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "U9ZdtyzfIc8a",
        "outputId": "48280898-7825-42ab-f291-01abb9e2cc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z^1_{i} \u0026= \\sum_{j}x_{j}U_{ji}\\\\\n",
            "y^1_{i} \u0026= \\text{relu}(z^1_{i})\\\\\n",
            "z^2 \u0026= \\sum_{i}y^1_{i}V_{i}\n"
          ]
        }
      ],
      "source": [
        "print(j2t.jax2tex(mlp, jnp.ones((3, 2)), jnp.ones((2,)), jnp.ones((3,))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k3M3Ol9FlUod"
      },
      "source": [
        "$$\n",
        "z^1_{i} = \\sum_{j}x_{j}U_{ji}\\\\\n",
        "y^1_{i} = \\text{relu}(z^1_{i})\\\\\n",
        "z^2 = \\sum_{i}y^1_{i}V_{i}\n",
        "$$\n",
        "\n",
        "Here we see equations defining an MLP that look a lot like what one might find in a textbook. Notice that as discussed above we are very explicit about indices and summation. Next we can look at the gradient,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 121
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 342,
          "status": "ok",
          "timestamp": 1597960784443,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "sqfbyeCr2i_0",
        "outputId": "bc7ea861-c126-44a7-bd7b-6ae4d7f66d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z^1_{i} \u0026= \\sum_{j}x_{j}U_{ji}\\\\\n",
            "\\delta y^1_{i} \u0026= 1.0V_{i}\\\\\n",
            "\\delta z^1_{i} \u0026= \\mathbbm 1_{z^1_{i}\u003e0.0}\\delta y^1_{i} + \\left(1 - \\mathbbm 1_{z^1_{i}\u003e0.0}\\right)0\\\\\n",
            "\\delta U_{ij} \u0026= \\delta z^1_{j}x_{i}\\\\\n",
            "y^1_{i} \u0026= \\text{relu}(z^1_{i})\\\\\n",
            "\\delta V_{i} \u0026= 1.0y^1_{i}\n"
          ]
        }
      ],
      "source": [
        "print(j2t.jax2tex(grad(mlp, argnums=(0, 1)), jnp.ones((3, 2)), jnp.ones((2,)), jnp.ones((3,))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jSU1Nu6Bmpnj"
      },
      "source": [
        "$$\n",
        "z^1_{i} = \\sum_{j}x_{j}U_{ji}\\\\\n",
        "\\delta y^1_{i} = 1.0V_{i}\\\\\n",
        "\\delta z^1_{i} = 1_{z^1_{i}\u003e0.0}\\delta y^1_{i} + \\left(1 - 1_{z^1_{i}\u003e0.0}\\right)0\\\\\n",
        "\\delta U_{ij} = \\delta z^1_{j}x_{i}\\\\\n",
        "y^1_{i} = \\text{relu}(z^1_{i})\\\\\n",
        "\\delta V_{i} = 1.0y^1_{i}\n",
        "$$\n",
        "\n",
        "Here we can see the forward pass (computing $z^1$ and $y^1$) as well as the backward pass (computing $\\delta y^1$, $\\delta z^1$, $\\delta U$, and $\\delta V$) written out in a relatively comprehensible form. \n",
        "\n",
        "A final feature that I have found useful is to define variables that are functions of some argument. Consider the following pair of functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 69
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 423,
          "status": "ok",
          "timestamp": 1597960860214,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "ZtAvIa1rFi3s",
        "outputId": "7850b1bf-2adf-4502-9a0c-07d515176186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z \u0026= {x}^{2}\\\\\n",
            "z \u0026= {y}^{2}\\\\\n",
            "f \u0026= zz\n"
          ]
        }
      ],
      "source": [
        "def g(x):\n",
        "  return j2t.tex_var(x ** 2, 'z')\n",
        "\n",
        "def f(x, y):\n",
        "  return g(x) * g(y)\n",
        "\n",
        "print(j2t.jax2tex(f, 1., 1.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "74dPJH2nojig"
      },
      "source": [
        "$$\n",
        "z = x^{2}\\\\\n",
        "z = y^{2}\\\\\n",
        "f = zz\n",
        "$$\n",
        "\n",
        "We see that the variable $z$ is ambigous and the dependence on $y$ and $x$ is suppressed. We can deal with this case properly by annotating that $z$ depends on $x$: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 69
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 361,
          "status": "ok",
          "timestamp": 1597960867481,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "wqJl4FdGOjEU",
        "outputId": "b0964a44-e467-46b7-d54f-8ea3e8df0217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z(x) \u0026= {x}^{2}\\\\\n",
            "z(y) \u0026= {y}^{2}\\\\\n",
            "f(x,y) \u0026= z(x)z(y)\n"
          ]
        }
      ],
      "source": [
        "def g(x):\n",
        "  return j2t.tex_var(x ** 2, 'z', depends_on=x)\n",
        "\n",
        "def f(x, y):\n",
        "  return g(x) * g(y)\n",
        "\n",
        "print(j2t.jax2tex(f, 1., 1.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jkbkKXpZo-Rk"
      },
      "source": [
        "$$\n",
        "z(x) = x^{2}\\\\\n",
        "z(y) = y^{2}\\\\\n",
        "f = z(x)z(y)\n",
        "$$\n",
        "\n",
        "Now we see that the two instances of $z$ depend explicitly on their arguments."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nittfHfNvRoS"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "jax2tex.ipynb",
      "provenance": [
        {
          "file_id": "1aQo2ykG_9boPi4jm2luPyGVSrmJ4Oh8h",
          "timestamp": 1597270609939
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
