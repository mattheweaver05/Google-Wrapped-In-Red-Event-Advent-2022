{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJZ6PzOv1kEa"
      },
      "source": [
        "# Progressive Distillation for Fast Sampling of Diffusion Models\n",
        "Code for the \u003ca href=\"https://openreview.net/forum?id=TIdIXIpzhoI\"\u003eICLR 2022 paper\u003c/a\u003e by Tim Salimans and Jonathan Ho.\n",
        "Model checkpoints to follow soon.\n",
        "\n",
        "Make sure to use a TPU when running this notebook, enabled via Runtime -\u003e Change runtime type -\u003e Hardware accelerator\n",
        "\n",
        "\u003ca href=\"https://colab.research.google.com/github/google-research/google_research/diffusion_distillation/blob/master/diffusion_distillation.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "**abstract**:\n",
        "Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.\n",
        "\n",
        "This notebook is intended as an easy way to get started with the Progressive Distillation algorithm. Reproducing the results from the paper exactly can be done using the hyperparameters in the provided config files, but this requires running at a larger scale and for longer than is practical in a notebook. We hope to be able to release the checkpoints for the trained model at a later time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPt2e2uRwyi6"
      },
      "source": [
        "![FID vs number of steps](../fid_steps_graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tEj-N-MOPkCV"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 13010,
          "status": "ok",
          "timestamp": 1643980536948,
          "user": {
            "displayName": "Tim Salimans",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmLFnDBh1aTFJV4sHGGSWCHed7xe4uq3o0QyUK=s64",
            "userId": "17371788552279899836"
          },
          "user_tz": -60
        },
        "id": "YsY4lIiZV0fn",
        "outputId": "339d4011-af47-4c5f-8966-500612afd8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checked out revision 8445.\n"
          ]
        }
      ],
      "source": [
        "# Download the diffusion_distillation repository \n",
        "!apt-get -qq install subversion\n",
        "!svn checkout https://github.com/google-research/google-research/trunk/diffusion_distillation\n",
        "!pip install -r diffusion_distillation/requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYae-ZOA1yaj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import functools\n",
        "import jax\n",
        "from jax.config import config\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as onp\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "from diffusion_distillation import diffusion_distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 20569,
          "status": "ok",
          "timestamp": 1643980563030,
          "user": {
            "displayName": "Tim Salimans",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmLFnDBh1aTFJV4sHGGSWCHed7xe4uq3o0QyUK=s64",
            "userId": "17371788552279899836"
          },
          "user_tz": -60
        },
        "id": "JzuoST_735Rc",
        "outputId": "39989620-3dd4-4e73-f0a5-696c6e2cd7b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grpc://10.115.199.154:8470\n"
          ]
        }
      ],
      "source": [
        "# configure JAX to use the TPU\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-s_VLDLuL6a"
      },
      "source": [
        "## Train a new diffusion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YuCpiSO32fW"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "config = diffusion_distillation.config.cifar_base.get_config()\n",
        "model = diffusion_distillation.model.Model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp0iRHeat6V5"
      },
      "outputs": [],
      "source": [
        "# init params \n",
        "state = jax.device_get(model.make_init_state())\n",
        "state = flax.jax_utils.replicate(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAdzcJ1stvZF"
      },
      "outputs": [],
      "source": [
        "# JIT compile training step\n",
        "train_step = functools.partial(model.step_fn, jax.random.PRNGKey(0), True)\n",
        "train_step = functools.partial(jax.lax.scan, train_step)  # for substeps\n",
        "train_step = jax.pmap(train_step, axis_name='batch', donate_argnums=(0,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSd8oEdgt160"
      },
      "outputs": [],
      "source": [
        "# build input pipeline\n",
        "total_bs = config.train.batch_size\n",
        "device_bs = total_bs // jax.device_count()\n",
        "train_ds = model.dataset.get_shuffled_repeated_dataset(\n",
        "    split='train',\n",
        "    batch_shape=(\n",
        "        jax.local_device_count(),  # for pmap\n",
        "        config.train.substeps,  # for lax.scan over multiple substeps\n",
        "        device_bs,  # batch size per device\n",
        "    ),\n",
        "    local_rng=jax.random.PRNGKey(0),\n",
        "    augment=True)\n",
        "train_iter = diffusion_distillation.utils.numpy_iter(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 674267,
          "status": "ok",
          "timestamp": 1643981313918,
          "user": {
            "displayName": "Tim Salimans",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmLFnDBh1aTFJV4sHGGSWCHed7xe4uq3o0QyUK=s64",
            "userId": "17371788552279899836"
          },
          "user_tz": -60
        },
        "id": "nEy9LsdhYtsz",
        "outputId": "3f97fb05-5c31-42d9-9932-1e5a7b8a06b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train/gnorm': 1.4675863981246948, 'train/loss': 0.39701905846595764}\n",
            "{'train/gnorm': 1.002144455909729, 'train/loss': 0.23647935688495636}\n",
            "{'train/gnorm': 1.164994239807129, 'train/loss': 0.1822689324617386}\n",
            "{'train/gnorm': 0.8901512026786804, 'train/loss': 0.1539679318666458}\n",
            "{'train/gnorm': 0.9623289108276367, 'train/loss': 0.15654230117797852}\n",
            "{'train/gnorm': 0.7790379524230957, 'train/loss': 0.1380912959575653}\n",
            "{'train/gnorm': 0.7820743322372437, 'train/loss': 0.1483442783355713}\n",
            "{'train/gnorm': 0.6346055865287781, 'train/loss': 0.13180819153785706}\n",
            "{'train/gnorm': 0.8465785980224609, 'train/loss': 0.13770630955696106}\n",
            "{'train/gnorm': 0.719518780708313, 'train/loss': 0.12717439234256744}\n"
          ]
        }
      ],
      "source": [
        "# run training\n",
        "for step in range(10):\n",
        "  batch = next(train_iter)\n",
        "  state, metrics = train_step(state, batch)\n",
        "  metrics = jax.device_get(flax.jax_utils.unreplicate(metrics))\n",
        "  metrics = jax.tree_map(lambda x: float(x.mean(axis=0)), metrics)\n",
        "  print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRMGXA_vxXiW"
      },
      "source": [
        "## Distill a trained diffusion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NSAchBAvu1q"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "config = diffusion_distillation.config.cifar_distill.get_config()\n",
        "model = diffusion_distillation.model.Model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-hzyGfo9Zzl"
      },
      "outputs": [],
      "source": [
        "# load the teacher params: todo\n",
        "# model.load_teacher_state(config.distillation.teacher_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvT6wPP46E5f"
      },
      "outputs": [],
      "source": [
        "# init student state\n",
        "init_params = diffusion_distillation.utils.copy_pytree(model.teacher_state.ema_params)\n",
        "optim = model.make_optimizer_def().create(init_params)\n",
        "state = diffusion_distillation.model.TrainState(\n",
        "    step=model.teacher_state.step,\n",
        "    optimizer=optim,\n",
        "    ema_params=diffusion_distillation.utils.copy_pytree(init_params),\n",
        "    num_sample_steps=model.teacher_state.num_sample_steps//2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afjP6y2h6jau"
      },
      "outputs": [],
      "source": [
        "# build input pipeline\n",
        "total_bs = config.train.batch_size\n",
        "device_bs = total_bs // jax.device_count()\n",
        "train_ds = model.dataset.get_shuffled_repeated_dataset(\n",
        "    split='train',\n",
        "    batch_shape=(\n",
        "        jax.local_device_count(),  # for pmap\n",
        "        config.train.substeps,  # for lax.scan over multiple substeps\n",
        "        device_bs,  # batch size per device\n",
        "    ),\n",
        "    local_rng=jax.random.PRNGKey(0),\n",
        "    augment=True)\n",
        "train_iter = diffusion_distillation.utils.numpy_iter(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qnjl_r36jdo"
      },
      "outputs": [],
      "source": [
        "steps_per_distill_iter = 10  # number of distillation steps per iteration of progressive distillation\n",
        "end_num_steps = 4  # eventual number of sampling steps we want to use \n",
        "while state.num_sample_steps \u003e= end_num_steps:\n",
        "\n",
        "  # compile training step\n",
        "  train_step = functools.partial(model.step_fn, jax.random.PRNGKey(0), True)\n",
        "  train_step = functools.partial(jax.lax.scan, train_step)  # for substeps\n",
        "  train_step = jax.pmap(train_step, axis_name='batch', donate_argnums=(0,))\n",
        "\n",
        "  # train the student against the teacher model\n",
        "  print('distilling teacher using %d sampling steps into student using %d steps'\n",
        "        % (model.teacher_state.num_sample_steps, state.num_sample_steps))\n",
        "  state = flax.jax_utils.replicate(state)\n",
        "  for step in range(steps_per_distill_iter):\n",
        "    batch = next(train_iter)\n",
        "    state, metrics = train_step(state, batch)\n",
        "    metrics = jax.device_get(flax.jax_utils.unreplicate(metrics))\n",
        "    metrics = jax.tree_map(lambda x: float(x.mean(axis=0)), metrics)\n",
        "    print(metrics)\n",
        "\n",
        "  # student becomes new teacher for next distillation iteration\n",
        "  model.teacher_state = jax.device_get(\n",
        "      flax.jax_utils.unreplicate(state).replace(optimizer=None))\n",
        "\n",
        "  # reset student optimizer for next distillation iteration\n",
        "  init_params = diffusion_distillation.utils.copy_pytree(model.teacher_state.ema_params)\n",
        "  optim = model.make_optimizer_def().create(init_params)\n",
        "  state = diffusion_distillation.model.TrainState(\n",
        "      step=model.teacher_state.step,\n",
        "      optimizer=optim,\n",
        "      ema_params=diffusion_distillation.utils.copy_pytree(init_params),\n",
        "      num_sample_steps=model.teacher_state.num_sample_steps//2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m3bdkYVyqhv"
      },
      "source": [
        "## Load a distilled model checkpoint and sample from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSxjJ5kNS0Kc"
      },
      "outputs": [],
      "source": [
        "# list all available distilled checkpoints\n",
        "# TODO: use cloud bucket in public version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE3bD54H_8kA"
      },
      "outputs": [],
      "source": [
        "# create imagenet model\n",
        "config = diffusion_distillation.config.imagenet64_base.get_config()\n",
        "model = diffusion_distillation.model.Model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHe7jXOdCVLE"
      },
      "outputs": [],
      "source": [
        "# load distilled checkpoint for 8 sampling steps\n",
        "loaded_params = diffusion_distillation.checkpoints.restore_from_path('/todo/imagenet_8', target=None)['ema_params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXYUUxGoFjz8"
      },
      "outputs": [],
      "source": [
        "# fix possible flax version errors\n",
        "ema_params = jax.device_get(model.make_init_state()).ema_params\n",
        "loaded_params = flax.core.unfreeze(loaded_params)\n",
        "loaded_params = jax.tree_map(\n",
        "    lambda x, y: onp.reshape(x, y.shape) if hasattr(y, 'shape') else x,\n",
        "    loaded_params,\n",
        "    flax.core.unfreeze(ema_params))\n",
        "loaded_params = flax.core.freeze(loaded_params)\n",
        "del ema_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dof1kKgjCcqM"
      },
      "outputs": [],
      "source": [
        "# sample from the model\n",
        "imagenet_classes = {'malamute': 249, 'siamese': 284, 'great_white': 2,\n",
        "                    'speedboat': 814, 'reef': 973, 'sports_car': 817,\n",
        "                    'race_car': 751, 'model_t': 661, 'truck': 867}\n",
        "labels = imagenet_classes['sports_car'] * jnp.ones((16,), dtype=jnp.int32)\n",
        "samples = jax.device_get(model.samples_fn(rng=jax.random.PRNGKey(0), labels=labels, params=loaded_params, num_steps=8)).astype(onp.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-NrZxvn6p0M"
      },
      "outputs": [],
      "source": [
        "# visualize samples\n",
        "padded_samples = onp.pad(samples, ((0,0), (1,1), (1,1), (0,0)), mode='constant', constant_values=255)\n",
        "nrows = int(onp.sqrt(padded_samples.shape[0]))\n",
        "ncols = padded_samples.shape[0]//nrows\n",
        "_, height, width, channels = padded_samples.shape\n",
        "img_grid = padded_samples.reshape(nrows, ncols, height, width, channels).swapaxes(1,2).reshape(height*nrows, width*ncols, channels)\n",
        "img = plt.imshow(img_grid)\n",
        "plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "diffusion_distillation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
